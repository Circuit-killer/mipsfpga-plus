<p><big><b>2. The theory of operation</b></big></p>

<p><big><b>2.1. MIPSfpga 2.0 hardware module hierarchy without Serial Loader Flow support</b></big></p>

<p><b><font color=blue>Figure 2.1</font></b> shows the basic hierarchy of synthesizable modules of MIPSfpga 2.0 for Digilent Nexys 4 DDR board with Xilinx Artix-7 FPGA and without Serial Loader hardware:</p>

<p><center><b><font color=blue>Figure 2.1</font></b></center></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/09/KgYa8BM.png" />

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/cache_locality.png" />

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/direct_mapped_cache.png" />


<img src="http://www.silicon-russia.com/wp-content/uploads/2016/10/mips_microaptiv_up_cache.png" />

<img src="http://www.silicon-russia.com/wp-content/uploads/2015/11/Screenshot-2015-11-24-21.39.48.png" />

https://www.amazon.com/Second-Morgan-Kaufmann-Computer-Architecture/dp/0120884216
https://www.amazon.com/Second-Morgan-Kaufmann-Computer-Architecture/dp/0120884216
See MIPS Run, Second Edition (The Morgan Kaufmann Series in Computer Architecture and Design) 2nd Edition

Computer Architecture, Fifth Edition: A Quantitative Approach
by David A. Patterson and John L. Hennessy

David Wentzlaff 2011 from Princeton University	

<blockquote>
<p><big><b>4.1 Caches and Cache Management</b></big></p>

<p>The cache's job is to keep a copy of memory data that has been recently
read or written, so it can be returned to the CPU quickly.  For L1 caches,
the read must complete in a fixed period of time to keep the pipeline
running.</p>

<p>MIPS CPUs always have separate L1 caches for instructions and data
(I-cache and D-cache, respectively) so that an instruction can be read and a
load or store done simultaneously.</p>

<p>. . . . .</p>

<p><big><b>4.2 How Caches Work</b></big></p>

<p>Conceptually, a cache is an associative memory, a chunk of storage where data
is deposited and can be found again using an arbitrary data pattern as a key. In
a cache, the key is the full memory address. Produce the same key back to an
associative memory and you'll get the same data back again. A real associative
memory will accept a new item using an arbitrary key, at least until it's full;
however, since a presented key has to be compared with every stored key simultaneously,
a genuine associative memory of any size is either hopelessly resource
hungry, slow, or both.</p>

<p>So how can we make a useful cache that is fast and efficient? Figure 4.1 shows
the basic layout of the simplest kind of cache, the direct-mapped cache used in
most MIPS CPUs up to the 1992 generation.</p>

<p>The direct-mapped arrangement uses a simple chunk of high-speed memory
(the cache store) indexed by enough low address bits to span its size. Each
line inside the cache store contains one or more words of data and a cache tag
field, which records the memory address where this data belongs.</p>

<p>On a read, the cache line is accessed, and the tag field is compared with the
higher addresses of the memory address; if the tag matches, we know we've got
the right data and have "hit" in the cache. Where there's more than one word in
the line, the appropriate word will be selected based on the very lowest address
bits.</p>

<p>If the tag doesn't match, we've missed and the data will be read from memory
and copied into the cache. The data that was previously held in the cache
is simply discarded and will need to be fetched from memory again if the CPU
references it.</p>

<center>
<p><b><font color=blue>FIGURE 4.1 Direct-mapped cache.</font></b></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/direct_mapped_cache.png" />
</center>

<p>A direct-mapped cache like this one has the property that, for any given
memory address, there is only one line in the cache store where that data
can be kept.<sup>1</sup> That might be good or bad; it's good because such a simple
structure will be fast and will allow us to run the whole CPU faster.  But
simplicity has its bad side too: If your program makes repeated reference to
two data items that happen to share the same cache location (presumably
because the low bits of their addresses happen to be close together), then
the two data items will keep pushing each other out of the cache and
efficiency will fall drastically.  A real associative memory wouldn't suffer
from this kind of thrashing, but it is too slow and expensive.</p>

<p>A common compromise is to use a two-way set-associative cacheâ€”which is
really just a matter of running two direct-mapped caches in parallel and
looking up memory locations in both of them, as shown in Figure 4.2.</p>

<sup>1.</sup> <small>In a fully associative memory, data associated with any given memory address (key) can be
stored anywhere; a direct-mapped cache is as far from being content addressable as a cache
store can be.</small>

<center>
<p><b><font color=blue>FIGURE 4.2 Two-way set-associative cache.</font></b></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/2_way_set_associative_cache.png" />
</center>

<p>Now we've got two chances of getting a hit on any address. Four-way setassociative
caches (where there are effectively four direct-mapped subcaches)
are also common in on-chip caches.</p>

<p>In a multiway cache there's more than one choice of the cache location to
be used in fixing up a cache miss, and thus more than one acceptable choice
of the cache line to be discarded. The ideal solution is probably to keep track
of accesses to cache lines and pick the "least recently used" ("LRU") line to be
replaced, but maintaining strict LRU order means updating LRU bits in every
cache line every time the cache is read. Moreover, keeping strict LRU information
in a more-than-four-way set-associative cache becomes impractical. Real
caches often use compromise algorithms like "least recently filled" to pick the
line to discard.</p>
</blockquote>
